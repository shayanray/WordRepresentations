# Task

Try out various methods to learn word representations building on Skip-grams and apply the learned word representations to a semantic task. Also implement two loss functions to train word vectors: (1) Cross entropy loss { the negative of the likelihood objective function we saw in class, and (2) Noise Contrastive Estimation { an alternate function that explicitly uses a set of k negative words.


## Config and Results
Best Configs:
--------------

	Cross Entropy:

		LearningRate:	1.0
		MaxNumSteps:	200001
		BatchSize:		128
		SkipWindow:		2
		NumSkips:		4


	NCE:
	
		NumNegativeSampled(k):	64
		LearningRate:			0.01
		MaxNumSteps:			500001
		BatchSize:				128
		SkipWindow:				2
		NumSkips:				4


Best DEV Accuracy corresponding to best configs above:
-----------------------------------------------

Cross Entropy:

	Generated by:                                     score_maxdiff.pl
	Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
	Test File:                                     predictions_cross_entropy_word_analogy_dev.txt
	Number of MaxDiff Questions:                      914
	Number of Least Illustrative Guessed Correctly:   300
	Number of Least Illustrative Guessed Incorrectly: 614
	Accuracy of Least Illustrative Guesses:            32.8%
	Number of Most Illustrative Guessed Correctly:    339
	Number of Most Illustrative Guessed Incorrectly:  575
	Accuracy of Most Illustrative Guesses:             37.1%
	Overall Accuracy:                                  35.0%


NCE:
	
	
	Generated by:                                     score_maxdiff.pl
	Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
	Test File:                                        predictions_nce_word_analogy_dev.txt
	Number of MaxDiff Questions:                      914
	Number of Least Illustrative Guessed Correctly:   302
	Number of Least Illustrative Guessed Incorrectly: 612
	Accuracy of Least Illustrative Guesses:            33.0%
	Number of Most Illustrative Guessed Correctly:    341
	Number of Most Illustrative Guessed Incorrectly:  573
	Accuracy of Most Illustrative Guesses:             37.3%
	Overall Accuracy:                                  35.2%



## Implementation

1. 	word2vec_basic.py: 
	
		generate_batch(data, batch_size, num_skips, skip_window)

		Logic:
		--------
		1.	Run an infinite loop until batch count = expected batch size matches
		2.	Get the current sliding window
				
				Within the current sliding window, keep track of num_skips_cnt

				IF the current word is not equal to the skip_window (note: skip_window actually translate to index position of context word)  then add to 'labels'
				
				ELSE
				add (window_size - 1) times to 'batches'

		3. Break from infinte loop in #1 IF the desired batch count has been reached


2.	loss_func.py

	cross_entropy_loss(inputs, true_w)

		Logic:
		--------

		1.	Simply use tensorflow API semantics to calculate A, B. return (A-B) tensor.
		2.	Calculate the common term ({u_w}^T v_c) using matmul
		3.	Calculate the exponential term  exp({u_w}^T v_c)
		4. 	Reuse #3, #4 in the corresponding expressions for A and B
		5. 	Based on FAQ suggestion added 1e-10 using calcualating log to avoid nans
				
	
	nce_loss(inputs, weights, biases, labels, sample, unigram_prob):

		Logic:
		--------

		1. 	Gather the shapes of all the inputs first for debugging purposes
		2.	Divide and Conquer approach taken:

				Solve for Term 1 (Labels)	

					1. solve for log(k.Pr(wo))
							
							convert unigram prob input to tensor and get the probabilities corresponding to the labels and then flatten

							multiply with 'k' and get log.

					2. solve for sigma(s(wo,wc)) = sigma((uTc.uo) + bo)

							solve for biases of outer words (bo)
							get nce weights for labels
							use that to get scores using matmul for outer words

					3. get the final first term translating the equation to corresponding tensorflow equivalent semantics (with sigmoids, log, add, subtract etc)


				Solve for Term 2 (for Negative Samples)

					1. solve for log(k.Pr(wx))
							
							reuse the unigram prob input tensor from above and get the probabilities corresponding to the samples and then flatten

							multiply with 'k' and get log.

					2. solve for sigma(s(wx,wc)) = sigma((uTc.ux) + bo)

							solve for biases of sample words (bx)
							get nce weights for samples
							use that to get scores using matmul for sample words

					3. get the final second term translating the equation to corresponding tensorflow equivalent semantics (with sigmoids, log, add, subtract etc)


				Final Objective Function term is obtained by adding Term1 and Term2 tensors from above and sending back the negative of that tensor.

	

3. 	word_analogy.py

		Logic:
		--------
		1. 	Using cosine similarity to measure the similarity between 2 vectors

		2.	Create a user-defined cosine similarity function to compute the similarity. (Dot Product of 2 vectors followed by the division of the product of their magnitudes)

		3.	Parse each line of the 'word_analogy_dev.txt' / 'word_analogy_test.txt'

		4. 	Separate out examples and choices and remove all extraneous spaces/quotes

		5. 	for each example pair, get the vector/embedding and compute the average similarity

		6.	for every choice pair, get the similarity and find its absolute difference between its value and average example similarity

		7. 	get the minimum value (least illustrative) and maximum value (most illustrative) to get the results

		8. 	write every line output to file on disk
